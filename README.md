# ğŸ•µï¸â€â™‚ï¸ Private_i â€” Personal Intelligence Desk

**Private_i** is a lightweight, local-first intelligence console for tracking fast-changing information â€” companies, roles, markets, and opportunities â€” in one place.  
It unifies the data-ingestion, normalization, and visualization layers from your other projects into a single control panel that runs anywhere.

---

## ğŸš€ Quick Overview
**Private_i** is not just a scraper.  
Itâ€™s an *intake â†’ normalize â†’ snapshot â†’ visualize* pipeline for your personal intelligence.

Think of it as a **self-hosted â€œinbox for signalsâ€**: run it, let it collect from multiple feeds, and browse results from one dashboard.

---

## ğŸ“ Project Structure

private_i/
â”œâ”€â”€ README.md # Documentation (this file)
â”œâ”€â”€ run_all.sh # Full pipeline runner
â”œâ”€â”€ app.py # Streamlit dashboard
â”œâ”€â”€ ingest_sources.py # Fetch and merge external feeds
â”œâ”€â”€ process_intel.py # Clean, tag, and score items
â”œâ”€â”€ snapshot_store.py # Store results in /data
â”œâ”€â”€ notion_sync.py # Optional Notion integration
â”œâ”€â”€ config/ # Source and tagging configuration
â”‚ â””â”€â”€ sources.yaml
â”œâ”€â”€ data/ # Local caches & historical snapshots
â”œâ”€â”€ secrets/ # .env with API keys (ignored by git)
â””â”€â”€ scripts/ # Operator scripts (publish, clean, demo)


---

## âš™ï¸ Installation
```bash
git clone https://github.com/your-username/private_i.git
cd private_i
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

Create a file at secrets/.env:

NOTION_API_KEY=your_key_here
OPENAI_API_KEY=optional_here
SLACK_WEBHOOK_URL=optional_here
PRIMARY_FEEDS=remotive,lever,greenhouse

Then run:

chmod +x run_all.sh
./run_all.sh

ğŸ§  How It Works
Stage	Description
1. Ingest	Pulls data from sources defined in config/sources.yaml
2. Process	Cleans titles, tags keywords (e.g. SE, AI, Remote)
3. Snapshot	Saves latest run in /data for comparison
4. Visualize	Displays interactive tables & filters in Streamlit
5. Publish	(Optional) Syncs results to Notion for daily review
ğŸª„ Features

    ğŸ” Multi-source intake â€” aggregate jobs or market data from several feeds

    ğŸ·ï¸ Smart tagging â€” apply SE/AI/remote labels automatically

    ğŸ“¦ Local snapshots â€” keep historical JSON/CSV runs

    ğŸ“Š Dashboard view â€” filter and explore your data visually

    ğŸ—’ï¸ Notion sync (optional) â€” export curated results

    ğŸ¤– AI summaries (optional) â€” GPT-based daily insights

    ğŸ› ï¸ Automation scripts â€” one-command demos & publish routines

ğŸ§° Example Usage

# Run full pipeline
./run_all.sh

# Fetch only
python ingest_sources.py

# Open dashboard
streamlit run app.py

# Sync to Notion
python notion_sync.py

ğŸ§© Integrations

    Notion API â€” export top results to your workspace

    OpenAI API â€” summarize new signals automatically

    Local filesystem â€” all data stays private and offline

    Streamlit â€” dashboard UI for review

ğŸ’¡ Example Scenarios

    Daily prospect / job review from multiple feeds

    Competitive tracking for target companies

    Personal SE pipeline and opportunity filtering

    Portfolio demo showing full-stack data orchestration

ğŸ§‘â€ğŸ’» Development Notes

    Language: Python 3.11+

    Frameworks: Streamlit + Pandas

    Config: YAML under config/

    Secrets: .env in secrets/

    Logging: CSV/JSON snapshots in data/

ğŸ”— Related Projects

    Coinbase Pipeline â€” Crypto trading analytics dashboard

    Job Pipeline â€” Job ingestion + Notion sync

    UX Job Scraper (ApplyPilot Ultra) â€” Multi-source job finder

    Jarvis â€” Terminal automation powering all orchestration

ğŸªª License

MIT â€” free to fork, modify, and deploy.
