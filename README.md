# ğŸ•µï¸â€â™‚ï¸ Private_i â€” IP Webcam â†’ Edge AI Camera Node

**Private_i** turns any Android phone running **IP Webcam** into a self-contained **AI camera node**.  
It pulls frames from your phone (`/shot.jpg`), runs **MobileNet-SSD** locally (CPU) via OpenCVâ€™s DNN, and serves a **Flask dashboard** with annotated video, live JSON summaries, and machine-readable endpoints.  
Itâ€™s a working demo of *â€œphone â†’ edge inference â†’ web UI â†’ JSON API.â€*

---

## ğŸš€ What It Does
- ğŸ“· Pulls frames from an Android IP Webcam (`CAMERA_URL` in `.env`)
- ğŸ§  Runs **MobileNet-SSD** object detection (`person`, `dog`, `bottle`, `car`, etc.)
- ğŸ—£ï¸ Generates sentences like: `I currently see 2 people, 1 bottle.`
- ğŸŒ Serves a live dashboard (`/`) plus machine endpoints:
  - `/health`
  - `/summary.json`
  - `/shot.jpg`
  - `/annotated.jpg`
  - `/video` (MJPEG stream)
- ğŸ’¾ Includes capture + bootstrap scripts
- ğŸ§© Stores examples in `captures/` for easy portfolio embedding

---

## ğŸ“ Repo Layout

Private_i/
â”œâ”€â”€ app.py # Flask app + detection + dashboard
â”œâ”€â”€ requirements.txt # Flask, OpenCV, requests, numpy, dotenv
â”œâ”€â”€ .env.example # CAMERA_URL, PORT, refresh interval
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ dev_run.sh # create venv, install deps, run app
â”‚ â”œâ”€â”€ fetch_models.sh # download MobileNetSSD model
â”‚ â””â”€â”€ capture_screens.sh # save annotated frames + JSON â†’ captures/
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ MobileNetSSD_deploy.prototxt
â”‚ â””â”€â”€ MobileNetSSD_deploy.caffemodel
â”œâ”€â”€ captures/ # sample screenshots + summaries
â”‚ â”œâ”€â”€ shot.jpg
â”‚ â”œâ”€â”€ annotated.jpg
â”‚ â””â”€â”€ summary.json
â””â”€â”€ bootstrap.sh # legacy setup script


---

## âš™ï¸ Setup & Run
```bash
git clone https://github.com/your-username/Private_i.git
cd Private_i
./scripts/dev_run.sh

That script:

    creates .venv

    installs dependencies

    fetches models if missing

    starts python app.py

Then visit:

http://localhost:5005/

Or configure manually:

cat <<'ENV' > .env
CAMERA_URL=http://192.168.0.42:8080
ANALYZE_EVERY=2
HOST=0.0.0.0
PORT=5005
ENV

ğŸ§  How It Works
Stage	Description
1. Frame grab	pulls /shot.jpg from CAMERA_URL
2. Detection	MobileNet-SSD DNN infers every few seconds
3. Annotation	draws boxes + labels using OpenCV
4. State	keeps latest frame + summary in memory
5. Presentation	Flask dashboard + REST endpoints
ğŸ§© Endpoints

    / â€” auto-refreshing dashboard

    /health â€” quick status JSON

    /summary.json â€” counts + English summary

    /shot.jpg â€” raw current frame

    /annotated.jpg â€” frame with boxes

    /video â€” MJPEG stream (auto-fallback if camera stream fails)

Example:

{
  "timestamp": 1730950000.123,
  "counts": { "person": 2, "bottle": 1 },
  "english": "I currently see 2 people, 1 bottle.",
  "camera": "http://192.168.0.42:8080"
}

ğŸ§ª Capture Evidence

Regenerate static captures for your portfolio:

./scripts/capture_screens.sh

ğŸ› ï¸ Model Fetching

If missing:

./scripts/fetch_models.sh

Downloads:

    models/MobileNetSSD_deploy.prototxt

    models/MobileNetSSD_deploy.caffemodel

ğŸ§‘â€ğŸ’» Requirements

flask==3.0.3
requests==2.32.3
numpy==1.26.4
opencv-python-headless==4.10.0.84
python-dotenv==1.0.1

ğŸ’¡ Portfolio Framing

Private_i demonstrates end-to-end edge AI:

    runs on CPU with no GPU

    turns any phone into a visual sensor

    exposes clean endpoints for other apps (e.g. Jarvis or dashboards)

    small enough to demo live in interviews

Itâ€™s your â€œreal-time computer-vision nodeâ€ â€” fast, portable, and entirely local.
ğŸªª License

MIT â€” free to fork, modify, and deploy.
